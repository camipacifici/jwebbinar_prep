{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651c89d8",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/STScI-MIRI/MRS-ExampleNB/raw/main/assets/banner1.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04a6ac",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# MIRI MRS Batch Processing Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2fb85c",
   "metadata": {},
   "source": [
    "**Author**: David Law, AURA Associate Astronomer, MIRI branch\n",
    "<br>\n",
    "**Last Updated**: September 12, 2022\n",
    "<br>\n",
    "**Pipeline Version**: 1.7.0+ (unreleased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988f765",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide a framework for batch processing of MIRI MRS data through all three pipeline stages.  Data is assumed to be located in two Observation folders (science and background) according to paths set up below.\n",
    "\n",
    "This example is set up to use observations from APT program 2732 (Stephan's Quintet ERO observations) using a standard 4-pt dither with dedicated background in all three grating settings.  Input data for this notebook can be obtained by downloading the 'uncal' files from MAST and placing them in directories set up below.  \n",
    "\n",
    "Changes:<br>\n",
    "Sep 1 2022: Add some commentary and example on how to use multicore processing in Detector 1<br>\n",
    "Sep 12 2022: Disable unnecessary cube/1d spectra production for individual science exposures in Spec 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d9868",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec45d0",
   "metadata": {},
   "source": [
    "1.<font color='white'>-</font>Configuration <a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to be changed here.\n",
    "# It should not be necessary to edit cells below this in general unless modifying pipeline processing steps.\n",
    "\n",
    "import sys,os, pdb\n",
    "os.environ[\"CRDS_PATH\"] = \"/home/shared/preloaded-crds/crds_cache/\"\n",
    "os.environ[\"CRDS_SERVER_URL\"] = \"https://jwst-crds-pub.stsci.edu\"\n",
    "os.environ[\"CRDS_CONTEXT\"] = \"jwst_0966.pmap\"\n",
    "\n",
    "# CRDS context (if overriding)\n",
    "#%env CRDS_CONTEXT jwst_0771.pmap\n",
    "\n",
    "# Point to where the uncalibrated FITS files are from the science observation\n",
    "input_dir = '/home/shared/preloaded-fits/pipeline_inflight/MRSNotebook1_inflight_data/'\n",
    "\n",
    "# Point to where you want the output science results to go\n",
    "output_dir = './MRS_data_output/'\n",
    "\n",
    "# Point to where the uncalibrated FITS files are from the background observation\n",
    "# If no background observation, leave this blank\n",
    "input_bgdir = '/home/shared/preloaded-fits/pipeline_inflight/MRSNotebook1_inflight_data2/'\n",
    "\n",
    "# Point to where the output background observations should go\n",
    "# If no background observation, leave this blank\n",
    "output_bgdir = './MRS_bg_output/'\n",
    "\n",
    "# Whether or not to run a given pipeline stage\n",
    "# Science and background are processed independently through det1+spec2, and jointly in spec3\n",
    "\n",
    "# Science processing\n",
    "dodet1=True\n",
    "dospec2=True\n",
    "dospec3=True\n",
    "\n",
    "# Background processing\n",
    "dodet1bg=True\n",
    "dospec2bg=True\n",
    "\n",
    "# If there is no background folder, ensure we don't try to process it\n",
    "if (input_bgdir == ''):\n",
    "    dodet1bg=False\n",
    "    dospec2bg=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab905400",
   "metadata": {},
   "source": [
    "Files can be found on Box at these two links:  \n",
    "https://stsci.box.com/s/9fii3tlaoc7usz9y2qnbho8ptukfqet9  \n",
    "https://stsci.box.com/s/7jim4oa4htgp3zdox5rdvk6f6jbfibjr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a015b",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0c995",
   "metadata": {},
   "source": [
    "2.<font color='white'>-</font>Imports and setup <a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use the entire available screen width for the notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7191bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "# Astropy utilities for opening FITS and ASCII files\n",
    "from astropy.io import fits\n",
    "from astropy.io import ascii\n",
    "from astropy.utils.data import download_file\n",
    "# Astropy utilities for making plots\n",
    "from astropy.visualization import (LinearStretch, LogStretch, ImageNormalize, ZScaleInterval)\n",
    "\n",
    "# Numpy for doing calculations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for making plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322125b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the base JWST package\n",
    "import jwst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JWST pipelines (encompassing many steps)\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels # JWST datamodels\n",
    "from jwst.associations import asn_from_list as afl # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base # Definition of a Lvl3 association file\n",
    "\n",
    "from stcal import dqflags # Utilities for working with the data quality (DQ) arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64fe0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output subdirectories to keep science data products organized\n",
    "# Note that the pipeline might complain about this as it is intended to work with everything in a single\n",
    "# directory, but it nonetheless works fine for the examples given here.\n",
    "det1_dir = os.path.join(output_dir, 'stage1/') # Detector1 pipeline outputs will go here\n",
    "spec2_dir = os.path.join(output_dir, 'stage2/') # Spec2 pipeline outputs will go here\n",
    "spec3_dir = os.path.join(output_dir, 'stage3/') # Spec3 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "if not os.path.exists(det1_dir):\n",
    "    os.makedirs(det1_dir)\n",
    "if not os.path.exists(spec2_dir):\n",
    "    os.makedirs(spec2_dir)\n",
    "if not os.path.exists(spec3_dir):\n",
    "    os.makedirs(spec3_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output subdirectories to keep background data products organized\n",
    "det1_bgdir = os.path.join(output_bgdir, 'stage1/') # Detector1 pipeline outputs will go here\n",
    "spec2_bgdir = os.path.join(output_bgdir, 'stage2/') # Spec2 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "if (output_bgdir != ''):\n",
    "    if not os.path.exists(det1_bgdir):\n",
    "        os.makedirs(det1_bgdir)\n",
    "    if not os.path.exists(spec2_bgdir):\n",
    "        os.makedirs(spec2_bgdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a7f86",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c9165",
   "metadata": {},
   "source": [
    "3.<font color='white'>-</font>Detector1 Pipeline <a class=\"anchor\" id=\"det1\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our data through the Detector1 pipeline to create Lvl2a data products (i.e., uncalibrated slope images).\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll define a function that will call the detector1 pipeline with our desired set of parameters\n",
    "# We won't enumerate the individual steps\n",
    "def rundet1(filename, outdir):\n",
    "    print(filename)\n",
    "    det1 = Detector1Pipeline() # Instantiate the pipeline\n",
    "    det1.output_dir = outdir # Specify where the output should go\n",
    "    \n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    #det1.dq_init.skip = False\n",
    "    #det1.saturation.skip = False\n",
    "    #det1.firstframe.skip = False\n",
    "    #det1.lastframe.skip = False\n",
    "    #det1.reset.skip = False\n",
    "    #det1.linearity.skip = False\n",
    "    #det1.rscd.skip = False\n",
    "    #det1.dark_current.skip = False\n",
    "    #det1.refpix.skip = False\n",
    "    #det1.jump.skip = False\n",
    "    #det1.ramp_fit.skip = False\n",
    "    #det1.gain_scale.skip = False\n",
    "    \n",
    "    # The jump and ramp fitting steps can benefit from multi-core processing, but this is off by default\n",
    "    # Turn them on here if desired by choosing how many cores to use (quarter, half, or all)\n",
    "    #det1.jump.maximum_cores='half'\n",
    "    #det1.ramp_fit.maximum_cores='half'\n",
    "    \n",
    "    # Bad pixel mask overrides\n",
    "    #det1.dq_init.override_mask = 'myfile.fits'\n",
    "\n",
    "    # Saturation overrides\n",
    "    #et1.saturation.override_saturation = 'myfile.fits'\n",
    "    \n",
    "    # Reset overrides\n",
    "    #det1.reset.override_reset = 'myfile.fits'\n",
    "        \n",
    "    # Linearity overrides\n",
    "    #det1.linearity.override_linearity = 'myfile.fits'\n",
    "\n",
    "    # RSCD overrides\n",
    "    #det1.rscd.override_rscd = 'myfile.fits'\n",
    "        \n",
    "    # DARK overrides\n",
    "    #det1.dark_current.override_dark = 'myfile.fits'\n",
    "        \n",
    "    # GAIN overrides\n",
    "    #det1.jump.override_gain = 'myfile.fits'\n",
    "    #det1.ramp_fit.override_gain = 'myfile.fits'\n",
    "                \n",
    "    # READNOISE overrides\n",
    "    #det1.jump.override_readnoise = 'myfile.fits'\n",
    "    #det1.ramp_fit.override_readnoise = 'myfile.fits'\n",
    "        \n",
    "    det1.save_results = True # Save the final resulting _rate.fits files\n",
    "    det1(filename) # Run the pipeline on an input list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f129790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look for input files of the form *uncal.fits from the science observation\n",
    "sstring = input_dir + 'jw*mirifu*uncal.fits'\n",
    "lvl1b_files = sorted(glob.glob(sstring))\n",
    "print('Found ' + str(len(lvl1b_files)) + ' science input files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "if dodet1:\n",
    "    for file in lvl1b_files:\n",
    "        rundet1(file, det1_dir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look for input files of the form *uncal.fits from the background observation\n",
    "sstring = input_bgdir + 'jw*mirifu*uncal.fits'\n",
    "lvl1b_files = sorted(glob.glob(sstring))\n",
    "print('Found ' + str(len(lvl1b_files)) + ' background input files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "if dodet1bg:\n",
    "    for file in lvl1b_files:\n",
    "        rundet1(file, det1_bgdir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d984a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81419572",
   "metadata": {},
   "source": [
    "4.<font color='white'>-</font>Spec2 Pipeline <a class=\"anchor\" id=\"spec2\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our data through the Spec2 pipeline in order to produce Lvl2b data products (i.e., calibrated slope images and quick-look data cubes and 1d spectra).  \n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will call the spec2 pipeline with our desired set of parameters\n",
    "# We'll list the individual steps just to make it clear what's running\n",
    "def runspec2(filename, outdir, nocubes=False):\n",
    "    spec2 = Spec2Pipeline()\n",
    "    spec2.output_dir = outdir\n",
    "\n",
    "    # Assign_wcs overrides\n",
    "    #spec2.assign_wcs.override_distortion = 'myfile.asdf'\n",
    "    #spec2.assign_wcs.override_regions = 'myfile.asdf'\n",
    "    #spec2.assign_wcs.override_specwcs = 'myfile.asdf'\n",
    "    #spec2.assign_wcs.override_wavelengthrange = 'myfile.asdf'\n",
    "\n",
    "    # Flatfield overrides\n",
    "    #spec2.flat.override_flat = 'myfile.fits'\n",
    "        \n",
    "    # Straylight overrides\n",
    "    #spec2.straylight.override_mrsxartcorr = 'myfile.fits'\n",
    "        \n",
    "    # Fringe overrides\n",
    "    #spec2.fringe.override_fringe = 'myfile.fits'\n",
    "    \n",
    "    # Photom overrides\n",
    "    #spec2.photom.override_photom = 'myfile.fits'\n",
    "\n",
    "    # Cubepar overrides\n",
    "    #spec2.cube_build.override_cubepar = 'myfile.fits'\n",
    "        \n",
    "    # Extract1D overrides\n",
    "    #spec2.extract1d.override_extract1d = 'myfile.asdf'\n",
    "    #spec2.extract1d.override_apcorr = 'myfile.asdf'\n",
    "        \n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    #spec2.assign_wcs.skip = False\n",
    "    #spec2.bkg_subtract.skip = True\n",
    "    #spec2.flat_field.skip = False\n",
    "    #spec2.srctype.skip = False\n",
    "    #spec2.straylight.skip = False\n",
    "    #spec2.fringe.skip = False\n",
    "    #spec2.photom.skip = False\n",
    "    #spec2.cube_build.skip = False\n",
    "    #spec2.extract_1d.skip = False\n",
    "    \n",
    "    # This nocubes option allows us to skip the cube building and 1d spectral extraction for individual\n",
    "    # science data frames, but run it for the background data (as the 1d spectra are needed later\n",
    "    # for the master background step in Spec3)\n",
    "    if (nocubes):\n",
    "        spec2.cube_build.skip = True\n",
    "        spec2.extract_1d.skip = True\n",
    "    \n",
    "    # Some cube building options\n",
    "    #spec2.cube_build.weighting='drizzle'\n",
    "    #spec2.cube_build.coord_system='ifualign' # If aligning cubes with IFU axes instead of sky\n",
    "      \n",
    "    spec2.save_results = True\n",
    "    spec2(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for uncalibrated science slope files from the Detector1 pipeline\n",
    "sstring = det1_dir + 'jw*mirifu*rate.fits'\n",
    "ratefiles = sorted(glob.glob(sstring))\n",
    "ratefiles=np.array(ratefiles)\n",
    "print('Found ' + str(len(ratefiles)) + ' input files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94665449",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec2:\n",
    "    for file in ratefiles:\n",
    "        runspec2(file, spec2_dir, nocubes=True)\n",
    "else:\n",
    "    print('Skipping Spec2 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad30204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for uncalibrated background slope files from the Detector1 pipeline\n",
    "sstring = det1_bgdir + 'jw*mirifu*rate.fits'\n",
    "ratefiles = sorted(glob.glob(sstring))\n",
    "ratefiles=np.array(ratefiles)\n",
    "print('Found ' + str(len(ratefiles)) + ' input files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83655aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec2bg:\n",
    "    for file in ratefiles:\n",
    "        runspec2(file, spec2_bgdir)\n",
    "else:\n",
    "    print('Skipping Spec2 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b351ac6",
   "metadata": {},
   "source": [
    "5.<font color='white'>-</font>Spec3 Pipeline: Default configuration (4 per-channel cubes)<a class=\"anchor\" id=\"spec3\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Here we'll run the Spec3 pipeline to produce a composite data cube from all dithered exposures.\n",
    "We will need to create an association file from all science and background data in order for the pipeline to use them appropriately.\n",
    "\n",
    "A word of caution: the data cubes created by the JWST pipeline are in SURFACE BRIGHTNESS units (MJy/steradian), not flux units.  What that means is that if you intend to sum spectra within an aperture you need to be sure to multiply by the pixel area in steradians first in order to get a spectrum in flux units (the PIXAR_SR keyword can be found in the SCI extension header).  This correction is already build into the pipeline Extract1D algorithm.\n",
    "\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a useful function to write out a Lvl3 association file from an input list\n",
    "# Note that any background exposures have to be of type x1d.\n",
    "def writel3asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "        \n",
    "    # Add background files to the association\n",
    "    nbg=len(bgfiles)\n",
    "    for ii in range(0,nbg):\n",
    "        asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort all of the input files\n",
    "\n",
    "# Science Files need the cal.fits files\n",
    "sstring = spec2_dir + 'jw*mirifu*cal.fits'\n",
    "calfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# Background Files need the x1d.fits files\n",
    "sstring = spec2_bgdir + 'jw*mirifu*x1d.fits'\n",
    "bgfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "print('Found ' + str(len(calfiles)) + ' science files to process')\n",
    "print('Found ' + str(len(bgfiles)) + ' background files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9918b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an association file that includes all of the different exposures\n",
    "# It has to be in the working directory\n",
    "asnfile=os.path.join('l3asn.json')\n",
    "if dospec3:\n",
    "    writel3asn(calfiles, bgfiles, asnfile, 'Level3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c035f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will call the spec3 pipeline with our desired set of parameters\n",
    "# This is designed to run on an association file\n",
    "def runspec3(filename):\n",
    "    # This initial setup is just to make sure that we get the latest parameter reference files\n",
    "    # pulled in for our files.  This is a temporary workaround to get around an issue with\n",
    "    # how this pipeline calling method works.\n",
    "    crds_config = Spec3Pipeline.get_config_from_reference(filename)\n",
    "    spec3 = Spec3Pipeline.from_config_section(crds_config)\n",
    "        \n",
    "    spec3.output_dir = spec3_dir\n",
    "    spec3.save_results = True\n",
    "    \n",
    "    # Cube building configuration options\n",
    "    # spec3.cube_build.output_file = 'bandcube' # Custom output name\n",
    "    # spec3.cube_build.output_type = 'band' # 'band', 'channel', or 'multi' type cube output\n",
    "    # spec3.cube_build.channel = '1' # Build everything from just channel 1 into a single cube (we could also choose '2','3','4', or 'ALL')\n",
    "    # spec3.cube_build.weighting = 'drizzle' # 'emsm' or 'drizzle'\n",
    "    # spec3.cube_build.coord_system = 'ifualign' # 'ifualign', 'skyalign', or 'internal_cal'\n",
    "    # spec3.cube_build.scale1 = 0.5 # Output cube spaxel scale (arcsec) in dimension 1 if setting it by hand\n",
    "    # spec3.cube_build.scale2 = 0.5 # Output cube spaxel scale (arcsec) in dimension 2 if setting it by hand\n",
    "    # spec3.cube_build.scalew = 0.002 # Output cube spaxel size (microns) in dimension 3 if setting it by hand\n",
    "    \n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    #spec3.assign_mtwcs.skip = False\n",
    "    #spec3.master_background.skip = True\n",
    "    #spec3.outlier_detection.skip = True\n",
    "    #spec3.mrs_imatch.skip = True\n",
    "    #spec3.cube_build.skip = False\n",
    "    #spec3.extract_1d.skip = False\n",
    "    \n",
    "    # Cubepar overrides\n",
    "    #spec3.cube_build.override_cubepar = 'myfile.fits'\n",
    "        \n",
    "    # Extract1D overrides\n",
    "    #spec3.extract1d.override_extract1d = 'myfile.asdf'\n",
    "    #spec3.extract1d.override_apcorr = 'myfile.asdf'\n",
    "\n",
    "    spec3(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ca0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec3:\n",
    "    runspec3(asnfile)\n",
    "else:\n",
    "    print('Skipping Spec3 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jdaviz import Cubeviz\n",
    "viz = Cubeviz()\n",
    "viz.load_data('./MRS_data_output/stage3/Level3_ch1-shortmediumlong_s3d.fits')\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e2553",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d16ec",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://www.stsci.edu/~dlaw/stsci_logo.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
